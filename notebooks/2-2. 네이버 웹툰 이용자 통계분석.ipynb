{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 필요 라이브러리 임포트"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 월요웹툰 웹페이지 정보 받아보기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "url = 'http://comic.naver.com/webtoon/weekdayList.nhn?week=mon'\n",
    "g = requests.get(url)\n",
    "g.content"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 월요웹툰 페이지 파싱하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "soup = BeautifulSoup(g.content, 'lxml')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 웹툰 목록 가져오기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "toon_list = soup.find('ul', attrs={'class': 'img_list'})\n",
    "toon_list = toon_list.find_all('li')\n",
    "len(toon_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 웹툰 링크 가져오기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def get_toon_link(item):\n",
    "    thumb = item.find('div', attrs={'class': 'thumb'})\n",
    "    link = thumb.find('a')\n",
    "    link = link['href']\n",
    "    return link\n",
    "\n",
    "toon_links = [get_toon_link(item) for item in toon_list]\n",
    "toon_links"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 웹툰 제목 가져오기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def get_toon_title(item):\n",
    "    return item.find('dl').find('dt').find('a').get_text()\n",
    "\n",
    "toon_titles = [get_toon_title(item) for item in toon_list]\n",
    "toon_titles"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 각 요일별 웹툰 링크 가져오기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "weekday_names = ['mon', 'tue', 'wed', 'thu', 'fri', 'sat', 'sun']\n",
    "base_url = 'http://comic.naver.com/webtoon/weekdayList.nhn?week='\n",
    "\n",
    "titles = []\n",
    "links = []\n",
    "\n",
    "for weekday in weekday_names:\n",
    "    url = base_url + weekday\n",
    "    g = requests.get(url)\n",
    "    soup = BeautifulSoup(g.content, 'lxml')\n",
    "    \n",
    "    toon_list = soup.find('ul', attrs={'class': 'img_list'})\n",
    "    toon_list = toon_list.find_all('li')\n",
    "    \n",
    "    toon_titles = [get_toon_title(item) for item in toon_list]\n",
    "    titles.extend(toon_titles)\n",
    "    \n",
    "    toon_links = [get_toon_link(item) for item in toon_list]\n",
    "    links.extend(toon_links)\n",
    "    \n",
    "    time.sleep(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 첫 번째 웹툰의 마지막 화 링크 알아보기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "link = links[0]\n",
    "link"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "toon_url = 'http://comic.naver.com' + link\n",
    "toon_url"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "g = requests.get(toon_url)\n",
    "soup = BeautifulSoup(g.content, 'lxml')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "ep_list = soup.find('table', attrs={'class': 'viewList'})\n",
    "ep_list = ep_list.find_all('tr')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "ep_list[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "ep_list[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "ep_list[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "ep_list[2].find('td').find('a')['href']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ep_list의 첫 번째 아이템은 에피소드 목록의 헤더이다. \n",
    "\n",
    "두 번째부터 에피소드 목록이 시작되는데, 실제 에피소드 목록이 시작되기 전에 광고가 삽입된 경우가 있다. \n",
    "\n",
    "광고를 나타내는 줄은 `<tr>` 태그의 attribute로 `class`를 가진다.\n",
    "\n",
    "이 정보를 사용해 목록의 각 아이템이 광고인지 아닌지를 알아낼 수 있다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def get_last_ep_link(ep_list):\n",
    "    for tr in ep_list[1:]:\n",
    "        if 'class' not in tr.attrs.keys():\n",
    "            return tr.find('td').find('a')['href']\n",
    "\n",
    "\n",
    "last_ep_link = get_last_ep_link(ep_list)\n",
    "last_ep_link"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 전체 웹툰 마지막 화 받아오기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "last_eps = []\n",
    "\n",
    "for i, link in enumerate(links):\n",
    "    print(i+1, '/', len(links))\n",
    "    \n",
    "    toon_url = 'http://comic.naver.com' + link\n",
    "    g = requests.get(toon_url)\n",
    "    soup = BeautifulSoup(g.content, 'lxml')\n",
    "    \n",
    "    ep_list = soup.find('table', attrs={'class': 'viewList'})\n",
    "    ep_list = ep_list.find_all('tr')\n",
    "    \n",
    "    last_ep = get_last_ep_link(ep_list)\n",
    "    last_eps.append(last_ep)\n",
    "    \n",
    "    time.sleep(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 첫 번째 웹툰의 마지막 화 회별 별점 및 참여자 수 받아오기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "last_ep = last_eps[0]\n",
    "last_ep = 'http://comic.naver.com' + last_ep\n",
    "\n",
    "g = requests.get(last_ep)\n",
    "soup = BeautifulSoup(g.content, 'lxml')\n",
    "\n",
    "rating = soup.find('span', attrs={'id': 'topPointTotalNumber'})\n",
    "rating = rating.get_text()\n",
    "\n",
    "num_partic = soup.find('span', attrs={'class': 'pointTotalPerson'})\n",
    "num_partic = num_partic.find('em').get_text()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 19금 웹툰의 경우"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "last_ep = 'http://comic.naver.com/webtoon/detail.nhn?titleId=530312&no=142&weekday=mon'\n",
    "g = requests.get(last_ep)\n",
    "soup = BeautifulSoup(g.content, 'lxml')\n",
    "print(soup.find('title'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 전체 웹툰 마지막 화 회별 별점 및 참여자 수 받아오기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "ratings = []\n",
    "num_partics = []\n",
    "\n",
    "for i, last_ep in enumerate(last_eps):\n",
    "    print(i + 1, '/', len(last_eps))\n",
    "    \n",
    "    last_ep = 'http://comic.naver.com' + last_ep\n",
    "    g = requests.get(last_ep)\n",
    "    soup = BeautifulSoup(g.content, 'lxml')\n",
    "    \n",
    "    title = soup.find('title').get_text()\n",
    "    if '로그인' in title:\n",
    "        ratings.append(-1)\n",
    "        num_partics.append(-1)\n",
    "        continue\n",
    "\n",
    "    rating = soup.find('span', attrs={'id': 'topPointTotalNumber'})\n",
    "    rating = rating.get_text()\n",
    "    ratings.append(float(rating))\n",
    "\n",
    "    num_partic = soup.find('span', attrs={'class': 'pointTotalPerson'})\n",
    "    num_partic = num_partic.find('em').get_text()\n",
    "    num_partics.append(int(num_partic))\n",
    "    \n",
    "    time.sleep(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 요일 분류하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "weekdays = [link[-3:] for link in links]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### pandas DataFrame으로 변환하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data_dict = {\n",
    "    'weekday': weekdays,\n",
    "    'title': titles,\n",
    "    'rating': ratings,\n",
    "    'num_partics': num_partics\n",
    "}\n",
    "\n",
    "import pandas as pd\n",
    "df = pd.DataFrame(data_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 미수집 웹툰 필터링"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df = df[df['num_partics'] > 0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Top 20 참여자 웹툰 확인하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df_top_20 = df.sort_values(by='num_partics', ascending=False)[:20]\n",
    "df_top_20"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Top 20 참여자 웹툰 시각화"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "matplotlib.rc('font', family='AppleGothic')\n",
    "matplotlib.style.use('ggplot')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df_top_20.plot.barh(x='title', \n",
    "                    y='num_partics', \n",
    "                    figsize=(15, 10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 요일별 별점 참여자 평균/표준편차 시각화"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "day_mean = df.groupby('weekday')['num_partics'].mean()\n",
    "day_std = df.groupby('weekday')['num_partics'].std()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df2 = pd.DataFrame(index=weekday_names)\n",
    "df2['mean'] = day_mean\n",
    "df2['std'] = day_std"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "df2.plot.bar(figsize=(10, 8))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [Root]",
   "language": "python",
   "name": "Python [Root]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
